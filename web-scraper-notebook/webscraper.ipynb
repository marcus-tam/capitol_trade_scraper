{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Dependencies\n",
    "from selenium import webdriver\n",
    "from selenium_stealth import stealth\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building WebScraper class\n",
    "class WebScraper:\n",
    "    def __init__(self, headless=True, interval=30, baseurl=\"https://www.capitoltrades.com/trades\"):\n",
    "        # Set up Chrome options\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        \n",
    "        # Disables the AutomationControlled feature, which helps to prevent websites from detecting that the browser is being controlled by automation software.\n",
    "        chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "        \n",
    "        # Runs Chrome in headless mode, which means it operates without a graphical user interface. This is useful for running automated tests or web scraping tasks on servers without a display.\n",
    "        if headless:\n",
    "            chrome_options.add_argument(\"--headless\")\n",
    "        \n",
    "        # Disables the sandbox security feature. This can help avoid certain security restrictions but should be used with caution as it reduces the security of the browser.\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        \n",
    "        # Disables the use of /dev/shm (shared memory) in Chrome. This can help prevent issues related to limited shared memory space, especially in containerized environments like Docker.\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        \n",
    "        # Disables GPU hardware acceleration. This can help avoid rendering issues and improve stability, particularly when running in headless mode.\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        \n",
    "        # Initialize the Chrome WebDriver with ChromeDriverManager\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        self.driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "        \n",
    "        # Apply stealth settings\n",
    "        stealth(self.driver,\n",
    "                languages=[\"en-US\", \"en\"],\n",
    "                vendor=\"Google Inc.\",\n",
    "                platform=\"Win32\",\n",
    "                webgl_vendor=\"Intel Inc.\",\n",
    "                renderer=\"Intel Iris OpenGL Engine\",\n",
    "                fix_hairline=True)\n",
    "        \n",
    "        self.wait = WebDriverWait(self.driver, 10)\n",
    "        self.baseurl = baseurl\n",
    "        self.seen_trades = set()\n",
    "        self.interval = interval\n",
    "\n",
    "    def navigate_to_site(self, baseurl=None):\n",
    "        if baseurl is None:\n",
    "            baseurl = self.baseurl\n",
    "        self.driver.get(baseurl)\n",
    "        time.sleep(5)\n",
    "        try:\n",
    "            self.wait.until(EC.presence_of_element_located((By.CLASS_NAME, \"site-main\")))  \n",
    "            print(\"Successfully loaded the page\")\n",
    "        except TimeoutException:\n",
    "            print(\"Failed to load the page\")\n",
    "            self.driver.quit()\n",
    "\n",
    "    def get_page_number(self):\n",
    "        try:\n",
    "            find_page_max = self.driver.find_element(By.XPATH, '/html/body/div/main/main/section/div[3]/div[2]/div[1]/p[1]/b[2]')\n",
    "            page_max = int(find_page_max.text)\n",
    "            return page_max\n",
    "        except TimeoutException:\n",
    "            print(\"Failed to locate the page maximum\")\n",
    "            self.driver.quit()\n",
    "            return None\n",
    "        except ValueError:\n",
    "            print(\"Failed to convert page number to integer\")\n",
    "            self.driver.quit()\n",
    "            return None\n",
    "\n",
    "    def extract_trades(self):\n",
    "        try:\n",
    "            self.wait.until(EC.presence_of_element_located((By.TAG_NAME, \"tbody\")))\n",
    "            table_xpath = '/html/body/div/main/main/section/div[3]/div[1]/div/table'\n",
    "            table = self.driver.find_element(By.XPATH, table_xpath)\n",
    "            table_data = table.text\n",
    "            return table_data\n",
    "        except TimeoutException:\n",
    "            print(\"Failed to locate the trades table\")\n",
    "            self.driver.quit()\n",
    "            return None\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get page max\n",
    "scraper = WebScraper(headless=False)\n",
    "scraper.navigate_to_site()\n",
    "page_max = scraper.get_page_number()\n",
    "\n",
    "all_trades = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get trade data\n",
    "try:\n",
    "    if page_max is not None:\n",
    "        for i in range(1, page_max + 1):\n",
    "            scraper.navigate_to_site(baseurl=f\"https://www.capitoltrades.com/trades?pageSize=100&page={i}\")\n",
    "            trades = scraper.extract_trades()\n",
    "            if trades:\n",
    "                all_trades.append(trades)\n",
    "\n",
    "            ## Put transformation code in here - then do a check to see if there are any multiples of Trade ID. If so stop the code and remove any duplicates.\n",
    "\n",
    "except Exception:\n",
    "    for trade in all_trades:\n",
    "        print(trade)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delimit the all_trades list by the \\n delimiter\n",
    "all_trades_data = [trade.split('\\n') for trade in all_trades]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the list of lists into a single list\n",
    "all_trades_flat = [item for sublist in all_trades_data for item in sublist]\n",
    "print(all_trades_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove headers\n",
    "cleaned_trades = []\n",
    "header_names = ['POLITICIAN', 'TRADED ISSUER', 'PUBLISHED', 'TRADED', 'FILED AFTER', 'OWNER', 'TYPE', 'SIZE', 'PRICE','Goto trade detail page.']\n",
    "for trade in all_trades_flat:\n",
    "    if trade not in header_names:\n",
    "        cleaned_trades.append(trade)\n",
    "\n",
    "cleaned_trades_length = len(cleaned_trades)\n",
    "print(cleaned_trades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_trades_table = np.array(cleaned_trades).reshape(-1, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set headers and create dataframe\n",
    "headers = ['Politician', 'Party', 'Traded Company Name', 'Traded Company Ticker', 'Published Date', 'Published Year', 'Traded On Date', 'Traded On Year', 'UoM of Time After Trade Filed', 'Time After Trade Filed', 'Trade Owner', 'Trade Type', 'Trade Size', 'Stock Price']\n",
    "cleaned_trades_df = pd.DataFrame(cleaned_trades_table, columns = headers)\n",
    "\n",
    "cleaned_trades_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining Published Date and Published Year into a single column then removing the original columns\n",
    "cleaned_trades_df[\"Published Datetime\"] = cleaned_trades_df[\"Published Date\"] + \" \" + cleaned_trades_df[\"Published Year\"]\n",
    "cleaned_trades_df.drop(columns=[\"Published Date\", \"Published Year\",\"UoM of Time After Trade Filed\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find values where word \"Yesterday\" is present - replace yesterday string with Yesterday's date\n",
    "yesterday_date = ( datetime.now()- timedelta(1)).strftime('%d, %b, %Y')\n",
    "cleaned_trades_df[\"Published Datetime\"].replace(\"Yesterday\", yesterday_date, inplace=True, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_trades_df[\"Published Datetime\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert \"Published Datetime\" to mm-dd-yyyy format, no times\n",
    "def date_convert(date_to_convert):\n",
    "    return datetime.strptime(date_to_convert, '%d, %b, %Y').strftime('%Y-%m-%d')\n",
    "\n",
    "def datetime_convert(date_convert):\n",
    "    return datetime.strptime(date_convert,'%H:%M %d, %b, %Y').strftime('%d %b %Y %H:%M')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each row in \"Published Datetime\"\n",
    "for index, row in cleaned_trades_df.iterrows():\n",
    "    date_str = row[\"Published Datetime\"]\n",
    "    try:\n",
    "        # Try to run date_convert - use at instead of loc if it breaks\n",
    "        cleaned_trades_df.loc[index, \"Published Datetime\"] = date_convert(date_str)\n",
    "    except ValueError:\n",
    "        try:\n",
    "            # If date_convert fails, try to run datetime_convert - use at instead of loc if it breaks\n",
    "            cleaned_trades_df.loc[index, \"Published Datetime\"] = datetime_convert(date_str)\n",
    "        except ValueError:\n",
    "            # If both fail, move to the next row\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining Traded Datetime data and removing original columns\n",
    "cleaned_trades_df[\"Traded Datetime\"] = cleaned_trades_df[\"Traded On Date\"] + \" \" + cleaned_trades_df[\"Traded On Year\"]\n",
    "cleaned_trades_df.drop(columns = ['Traded On Date', 'Traded On Year'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Time After Trade Filed to Int\n",
    "cleaned_trades_df[\"Time After Trade Filed\"] = cleaned_trades_df[\"Time After Trade Filed\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Time After Trade Filed to date, rename column\n",
    "for index, row in cleaned_trades_df.iterrows():\n",
    "    days_ago = row[\"Time After Trade Filed\"]\n",
    "    cleaned_trades_df.loc[index, \"Time After Trade Filed\"] = (datetime.now() - timedelta(days=days_ago)).strftime('%d %b %Y')\n",
    "\n",
    "cleaned_trades_df.rename(columns = {\"Time After Trade Filed\": \"Trade Filed Date\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_trades_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_trades_df[\"Trade ID\"] = (\n",
    "    cleaned_trades_df[\"Politician\"].astype(str) + \"_\" +\n",
    "    cleaned_trades_df[\"Party\"].astype(str) + \"_\" +\n",
    "    cleaned_trades_df[\"Traded Company Name\"].astype(str) + \"_\" +\n",
    "    cleaned_trades_df[\"Traded Company Ticker\"].astype(str) + \"_\" +\n",
    "    cleaned_trades_df[\"Trade Filed Date\"].astype(str) + \"_\" +\n",
    "    cleaned_trades_df[\"Trade Owner\"].astype(str) + \"_\" +\n",
    "    cleaned_trades_df[\"Trade Type\"].astype(str) + \"_\" +\n",
    "    cleaned_trades_df[\"Trade Size\"].astype(str) + \"_\" +\n",
    "    cleaned_trades_df[\"Stock Price\"].astype(str) + \"_\" +\n",
    "    cleaned_trades_df[\"Published Datetime\"].astype(str) + \"_\" +\n",
    "    cleaned_trades_df[\"Traded Datetime\"].astype(str)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_trades_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_trades_df[\"Trade ID\"].value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm and close\n",
    "input(\"Press any key to close the browser\")\n",
    "scraper.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
