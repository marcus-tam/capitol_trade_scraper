{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Dependencies\n",
    "from selenium import webdriver\n",
    "from selenium_stealth import stealth\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building WebScraper class\n",
    "class WebScraper:\n",
    "    def __init__(self, headless=True, interval=30, baseurl=\"https://www.capitoltrades.com/trades\"):\n",
    "        # Set up Chrome options\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        \n",
    "        # Disables the AutomationControlled feature, which helps to prevent websites from detecting that the browser is being controlled by automation software.\n",
    "        chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "        \n",
    "        # Runs Chrome in headless mode, which means it operates without a graphical user interface. This is useful for running automated tests or web scraping tasks on servers without a display.\n",
    "        if headless:\n",
    "            chrome_options.add_argument(\"--headless\")\n",
    "        \n",
    "        # Disables the sandbox security feature. This can help avoid certain security restrictions but should be used with caution as it reduces the security of the browser.\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        \n",
    "        # Disables the use of /dev/shm (shared memory) in Chrome. This can help prevent issues related to limited shared memory space, especially in containerized environments like Docker.\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        \n",
    "        # Disables GPU hardware acceleration. This can help avoid rendering issues and improve stability, particularly when running in headless mode.\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        \n",
    "        # Initialize the Chrome WebDriver with ChromeDriverManager\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        self.driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "        \n",
    "        # Apply stealth settings\n",
    "        stealth(self.driver,\n",
    "                languages=[\"en-US\", \"en\"],\n",
    "                vendor=\"Google Inc.\",\n",
    "                platform=\"Win32\",\n",
    "                webgl_vendor=\"Intel Inc.\",\n",
    "                renderer=\"Intel Iris OpenGL Engine\",\n",
    "                fix_hairline=True)\n",
    "        \n",
    "        self.wait = WebDriverWait(self.driver, 10)\n",
    "        self.baseurl = baseurl\n",
    "        self.seen_trades = set()\n",
    "        self.interval = interval\n",
    "\n",
    "    def navigate_to_site(self, baseurl=None):\n",
    "        if baseurl is None:\n",
    "            baseurl = self.baseurl\n",
    "        self.driver.get(baseurl)\n",
    "        time.sleep(5)\n",
    "        try:\n",
    "            self.wait.until(EC.presence_of_element_located((By.CLASS_NAME, \"site-main\")))  \n",
    "            print(\"Successfully loaded the page\")\n",
    "        except TimeoutException:\n",
    "            print(\"Failed to load the page\")\n",
    "            self.driver.quit()\n",
    "\n",
    "    def get_page_number(self):\n",
    "        try:\n",
    "            find_page_max = self.driver.find_element(By.XPATH, '/html/body/div/main/main/section/div[3]/div[2]/div[1]/p[1]/b[2]')\n",
    "            page_max = int(find_page_max.text)\n",
    "            return page_max\n",
    "        except TimeoutException:\n",
    "            print(\"Failed to locate the page maximum\")\n",
    "            self.driver.quit()\n",
    "            return None\n",
    "        except ValueError:\n",
    "            print(\"Failed to convert page number to integer\")\n",
    "            self.driver.quit()\n",
    "            return None\n",
    "\n",
    "    def extract_trades(self):\n",
    "        try:\n",
    "            self.wait.until(EC.presence_of_element_located((By.TAG_NAME, \"tbody\")))\n",
    "            table_xpath = '/html/body/div/main/main/section/div[3]/div[1]/div/table'\n",
    "            table = self.driver.find_element(By.XPATH, table_xpath)\n",
    "            table_data = table.text\n",
    "            return table_data\n",
    "        except TimeoutException:\n",
    "            print(\"Failed to locate the trades table\")\n",
    "            self.driver.quit()\n",
    "            return None\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get page max\n",
    "scraper = WebScraper(headless=False)\n",
    "scraper.navigate_to_site()\n",
    "page_max = scraper.get_page_number()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "from db.utils import DatabaseConnector\n",
    "db = DatabaseConnector(host=\"localhost\", database=\"mydb\", user=\"myuser\", password=\"mypassword\", port=5432)\n",
    "db.connect()\n",
    "last_row = db.get_last_row()\n",
    "last_row = last_row[1:]\n",
    "all_trades = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_row_df = pd.DataFrame(last_row).T\n",
    "last_row_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_to_check = last_row_df.iloc[0,-1]\n",
    "value_to_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(last_row_df.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get trade data\n",
    "try:\n",
    "    if page_max is not None:\n",
    "        for i in range(1, page_max + 1):\n",
    "            scraper.navigate_to_site(baseurl=f\"https://www.capitoltrades.com/trades?pageSize=100&page={i}\")\n",
    "            trades = scraper.extract_trades()\n",
    "            print(f\"Extracted trades from page {i}\")\n",
    "            if trades:\n",
    "                all_trades.append(trades)\n",
    "                print(\"Received trades\")\n",
    "\n",
    "            # Split data data\n",
    "            all_trades_data = [trade.split('\\n') for trade in all_trades]\n",
    "            print(\"Split data\")\n",
    "\n",
    "            # Flatten list of lists into a single list\n",
    "            all_trades_flat = [item for sublist in all_trades_data for item in sublist]\n",
    "\n",
    "            # Remove headers\n",
    "            cleaned_trades = []\n",
    "            header_names = ['POLITICIAN', 'TRADED ISSUER', 'PUBLISHED', 'TRADED', 'FILED AFTER', 'OWNER', 'TYPE', 'SIZE', 'PRICE','Goto trade detail page.']\n",
    "            for trade in all_trades_flat:\n",
    "                if trade not in header_names:\n",
    "                    cleaned_trades.append(trade)\n",
    "\n",
    "            cleaned_trades_length = len(cleaned_trades)\n",
    "\n",
    "            # Reshape data to 14 columns to fit formatting schema\n",
    "            cleaned_trades_table = np.array(cleaned_trades).reshape(-1, 14)\n",
    "\n",
    "            # Create header names and dataframe\n",
    "            headers = ['Politician', 'Party', 'Traded Company Name', 'Traded Company Ticker', 'Published Date', 'Published Year', 'Traded On Date', 'Traded On Year', 'UoM of Time After Trade Filed', 'Time After Trade Filed', 'Trade Owner', 'Trade Type', 'Trade Size', 'Stock Price']\n",
    "            cleaned_trades_df = pd.DataFrame(cleaned_trades_table, columns = headers)\n",
    "\n",
    "            cleaned_trades_df.tail()\n",
    "\n",
    "            ## Data transformation\n",
    "            # Combining Published Date and Published Year into a single column then removing the original columns\n",
    "            cleaned_trades_df[\"Published Datetime\"] = cleaned_trades_df[\"Published Date\"] + \" \" + cleaned_trades_df[\"Published Year\"]\n",
    "            cleaned_trades_df.drop(columns=[\"Published Date\", \"Published Year\",\"UoM of Time After Trade Filed\"], inplace=True)\n",
    "\n",
    "            # Find values where word \"Yesterday\" is present - replace yesterday string with Yesterday's date\n",
    "            yesterday_date = ( datetime.now()- timedelta(1)).strftime('%d, %b, %Y')\n",
    "            cleaned_trades_df[\"Published Datetime\"].replace(\"Yesterday\", yesterday_date, inplace=True, regex=True)\n",
    "\n",
    "            # Find values where word \"Today\" is present - replace today string with Yesterday's date\n",
    "            today_date = (datetime.now()).strftime('%d, %b, %Y')\n",
    "            cleaned_trades_df[\"Published Datetime\"].replace(\"Today\", today_date, inplace=True, regex=True)\n",
    "\n",
    "            # Convert \"Published Datetime\" to mm-dd-yyyy format, no times\n",
    "            def date_convert(date_to_convert):\n",
    "                return datetime.strptime(date_to_convert, '%d, %b, %Y').strftime('%Y-%m-%d')\n",
    "\n",
    "            def datetime_convert(date_convert):\n",
    "                return datetime.strptime(date_convert,'%H:%M %d, %b, %Y').strftime('%Y-%m-%d')\n",
    "            \n",
    "            \n",
    "            # Iterate through each row in \"Published Datetime\" to remove time from Published Datetime data (not relevant)\n",
    "            for index, row in cleaned_trades_df.iterrows():\n",
    "                date_str = row[\"Published Datetime\"]\n",
    "                try:\n",
    "                    # Try to run date_convert - use at instead of loc if it breaks\n",
    "                    cleaned_trades_df.loc[index, \"Published Datetime\"] = date_convert(date_str)\n",
    "                except ValueError:\n",
    "                    try:\n",
    "                        # If date_convert fails, try to run datetime_convert - use at instead of loc if it breaks\n",
    "                        cleaned_trades_df.loc[index, \"Published Datetime\"] = datetime_convert(date_str)\n",
    "                    except ValueError:\n",
    "                        # If both fail, move to the next row\n",
    "                        continue\n",
    "\n",
    "            \n",
    "            # Combining Traded Datetime data and removing original columns\n",
    "            cleaned_trades_df[\"Traded Datetime\"] = cleaned_trades_df[\"Traded On Date\"] + \" \" + cleaned_trades_df[\"Traded On Year\"]\n",
    "            cleaned_trades_df.drop(columns = ['Traded On Date', 'Traded On Year'], inplace=True)\n",
    "\n",
    "            # Convert Time After Trade Filed to Int\n",
    "            cleaned_trades_df[\"Time After Trade Filed\"] = cleaned_trades_df[\"Time After Trade Filed\"].astype(int)\n",
    "\n",
    "            # Convert Time After Trade Filed to date, rename column\n",
    "            for index, row in cleaned_trades_df.iterrows():\n",
    "                days_ago = row[\"Time After Trade Filed\"]\n",
    "                cleaned_trades_df.loc[index, \"Time After Trade Filed\"] = (datetime.now() - timedelta(days=days_ago)).strftime('%d %b %Y')\n",
    "            \n",
    "            cleaned_trades_df.rename(columns = {\"Time After Trade Filed\": \"Trade Filed Date\"}, inplace=True)\n",
    "\n",
    "            # Create Trade ID for checking duplicates\n",
    "            cleaned_trades_df[\"Trade ID\"] = (\n",
    "            cleaned_trades_df[\"Politician\"].astype(str) + \"_\" +\n",
    "            cleaned_trades_df[\"Party\"].astype(str) + \"_\" +\n",
    "            cleaned_trades_df[\"Traded Company Name\"].astype(str) + \"_\" +\n",
    "            cleaned_trades_df[\"Traded Company Ticker\"].astype(str) + \"_\" +\n",
    "            cleaned_trades_df[\"Trade Filed Date\"].astype(str) + \"_\" +\n",
    "            cleaned_trades_df[\"Trade Owner\"].astype(str) + \"_\" +\n",
    "            cleaned_trades_df[\"Trade Type\"].astype(str) + \"_\" +\n",
    "            cleaned_trades_df[\"Trade Size\"].astype(str) + \"_\" +\n",
    "            cleaned_trades_df[\"Stock Price\"].astype(str) + \"_\" +\n",
    "            cleaned_trades_df[\"Published Datetime\"].astype(str) + \"_\" +\n",
    "            cleaned_trades_df[\"Traded Datetime\"].astype(str)\n",
    "        )\n",
    "            \n",
    "            ## If you can match an index with last row, return true and pull everything on that row and below\n",
    "            # Check if the value in last_row_df[11] is in cleaned_trades_df[\"Trade ID\"]\n",
    "            value_to_check = last_row_df.iloc[0,-1]\n",
    "            is_value_present = value_to_check in cleaned_trades_df[\"Trade ID\"].values\n",
    "            print(f\"Is the value '{value_to_check}' present in 'Trade ID' column? {is_value_present}\")\n",
    "\n",
    "            if is_value_present:\n",
    "                # Find the last occurrence of the value\n",
    "                row_above_first_occurence_index = (cleaned_trades_df[cleaned_trades_df[\"Trade ID\"] == value_to_check].index[0]) - 1\n",
    "\n",
    "                # Cut the DataFrame to include all values above the last found match\n",
    "                cleaned_trades_df = cleaned_trades_df.loc[:max(row_above_first_occurence_index,0)]\n",
    "\n",
    "                print(\"DataFrame after cutting:\")\n",
    "                print(cleaned_trades_df)\n",
    "\n",
    "\n",
    "            if i == page_max + 1:\n",
    "                print(\"Page max reached\")\n",
    "                exit\n",
    "                \n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    for trade in all_trades:\n",
    "        print(trade)\n",
    "    exit\n",
    "exit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
